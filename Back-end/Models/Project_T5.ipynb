{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEIOMCUYk6nr",
        "outputId": "5a4b05d0-9528-432f-c658-a2f0f18bfd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4armtw1Sk_Jt"
      },
      "outputs": [],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voQ2eGw02atY",
        "outputId": "56ef720a-7393-4757-f767-82739d15b3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-yZUm5uvm9q",
        "outputId": "adb49512-008e-4ac6-d8fc-d80be33784f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq transformers datasets accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHM9TrB5D0dv",
        "outputId": "4fb22790-9d97-4c25-b5e3-3c74ccbd2ef4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/472.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m471.0/472.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install quietly\n",
        "!pip install huggingface_hub -qq"
      ],
      "metadata": {
        "id": "qWukDMTwbzmn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_D5otMGFUiZ",
        "outputId": "29c6024c-279a-457b-fe8c-7cc4b10b8dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "from typing import List\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HF_KEY_FINE = userdata.get(\"HF_KEY_FINE\")\n",
        "# login(token=os.environ.get(\"HF_KEY_FINE\"))"
      ],
      "metadata": {
        "id": "KOmLBMcydIfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_ACCESS_TOKEN')"
      ],
      "metadata": {
        "id": "_XSQ3SqXdqOB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZltpasEXmuXD"
      },
      "outputs": [],
      "source": [
        "# Function to generate text using OpenAI API\n",
        "def openai_generate_text(prompt, api_key, model=\"gpt-4\", max_tokens=100, temperature=0.7):\n",
        "    openai.api_key = api_key\n",
        "\n",
        "    try:\n",
        "        # Call OpenAI's GPT model to generate text using Chat API\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "\n",
        "        # Extract generated text from response\n",
        "        generated_text = response['choices'][0]['message']['content'].strip()\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "FkkazscbD8oN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemma_generate_text(prompt, model=\"google/gemma-2b-it\", max_tokens=100,):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Use pipeline API\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        model_kwargs={\"torch_dtype\": \"auto\"},\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # Delete large objects\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate the text with a maximum of 256 new tokens\n",
        "    response = pipe(prompt, max_new_tokens=max_tokens)\n",
        "\n",
        "    # Extract the generated text from the response\n",
        "    generated_response = response[0][\"generated_text\"].strip()\n",
        "    return generated_response"
      ],
      "metadata": {
        "id": "fv66tOD3BO5N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MWKdCHGbmrVq"
      },
      "outputs": [],
      "source": [
        "# Load the T5 model and tokenizer\n",
        "def load_model_and_tokenizer(model_path):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Gemma model and tokenizer\n",
        "def load_gemma_model_and_tokenizer(model_path):\n",
        "    login(token=HF_TOKEN)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    return tokenizer, model"
      ],
      "metadata": {
        "id": "s5aWVnE7BIbl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GADAEbHambTz"
      },
      "outputs": [],
      "source": [
        "# Generate text using the model based on the given prompt\n",
        "def generate_text(prompt, tokenizer, model):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids\n",
        "    outputs = model.generate(input_ids, max_length=512, num_return_sequences=1)\n",
        "    # outputs = model.generate(input_ids, max_length=512, num_return_sequences=1, encoder_outputs=None)\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "myWRXTqrmkUg"
      },
      "outputs": [],
      "source": [
        "# Function to extract relevant key words/phrases from response\n",
        "def extract_key_words(response: str) -> List[str]:\n",
        "    key_words = [word.strip() for word in response.split() if len(word) > 3]\n",
        "    return key_words\n",
        "\n",
        "# Function to extract key words/phrases from job description\n",
        "def extract_key_phrases(job_description: str) -> List[str]:\n",
        "    key_words = [word.strip() for word in job_description.split() if len(word) > 3]\n",
        "    return key_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "NcT6orqVmh2p"
      },
      "outputs": [],
      "source": [
        "# Function to find matching experience in resume text based on job description key phrases\n",
        "def match_experience_to_job(key_phrases: List[str], resume_txt: str) -> List[str]:\n",
        "    matched_experiences = set()\n",
        "    sentences = resume_txt.split(\". \")\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for phrase in key_phrases:\n",
        "            if phrase.lower() in sentence.lower():\n",
        "                matched_experiences.add(sentence.strip())\n",
        "                break\n",
        "    return list(matched_experiences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GEifRo1UmgRy"
      },
      "outputs": [],
      "source": [
        "# Format the matched experiences into bullet points\n",
        "def format_bullet_points(experiences: List[str]) -> str:\n",
        "    if not experiences:\n",
        "        return \"No matching experience found.\"\n",
        "    bullet_points = \"\\n\".join([f\"- {exp}\" for exp in experiences])\n",
        "    return bullet_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bO6ZhZWp2TaH"
      },
      "outputs": [],
      "source": [
        "#Summarize Resume\n",
        "import pdfplumber\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    all_text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            all_text += page.extract_text()\n",
        "    return all_text.strip()\n",
        "\n",
        "# Function to summarize the extracted text using HuggingFace T5 model\n",
        "def summarize_text(text: str, max_length: int = 150) -> str:\n",
        "    summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
        "    summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Using OpenAI API for summarization\n",
        "def summarize_experience(text: str, api_key: str, model=\"gpt-4\"):\n",
        "    openai.api_key = api_key\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            # messages=[{\"role\": \"user\", \"content\": f\"Summarize the work experience in this resume:\\n{text}\"}],\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"Summarize the work experience for each job position held in this resume in reverse chronological order:\\n{text}\"}],\n",
        "            max_tokens=300\n",
        "        )\n",
        "        summary = response['choices'][0]['message']['content'].strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract and summarize resume from PDF\n",
        "def process_resume(pdf_path: str, selection: str, api_key: str):\n",
        "    summarized_text = None\n",
        "    resume_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    if selection == \"openai\":\n",
        "        summarized_text = summarize_experience(resume_text, api_key)\n",
        "    elif selection == \"huggingface\":\n",
        "        summarized_text = summarize_text(resume_text)\n",
        "    else:\n",
        "        print(\"Invalid selection. Please choose 'openai' or 'huggingface'.\")\n",
        "\n",
        "    print(\"\\nSummarized Resume Text:\\n\", summarized_text)\n",
        "    return summarized_text\n",
        "\n",
        "\n",
        "pdf_path = \"Pauline Korukundo CV.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt, Inches\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "from docx.shared import Cm\n",
        "import re\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Y5HSSpuX7MMz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SpaCy to Extract Job Titles\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to check if a line is a job title using NER\n",
        "def is_job_title(line: str) -> bool:\n",
        "    doc = nlp(line)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"ORG\" or ent.label_ == \"JOB_TITLE\":\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "zkbRzlDt33zc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl9Jr_tkv1Sa"
      },
      "source": [
        "Prepare output document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ZiANtG7av0RW"
      },
      "outputs": [],
      "source": [
        "def create_resume_doc(bullet_points: str, education: str, profile: str, file_name: str = \"Tailored_Resume.docx\"):\n",
        "    # Create a new Document\n",
        "    doc = Document()\n",
        "\n",
        "    # Set the document to A4 size\n",
        "    section = doc.sections[0]\n",
        "    section.page_height = Cm(29.7)\n",
        "    section.page_width = Cm(21.0)\n",
        "\n",
        "    # Set margins for A4\n",
        "    section.left_margin = Cm(2)\n",
        "    section.right_margin = Cm(2)\n",
        "    section.top_margin = Cm(2)\n",
        "    section.bottom_margin = Cm(2)\n",
        "\n",
        "    # Title of the Resume\n",
        "    title = doc.add_heading(\"RESUME\", level=1)\n",
        "    title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
        "\n",
        "    # Profile Section\n",
        "\n",
        "    if profile:\n",
        "        doc.add_paragraph(\"Profile\", style='Heading 2')\n",
        "        achievement_lines = profile.split(\"\\n\")\n",
        "        for line in achievement_lines:\n",
        "            if line.strip():\n",
        "                doc.add_paragraph(line.strip(), style='List Bullet')\n",
        "\n",
        "    # Add Experience Section to document\n",
        "    doc.add_paragraph(\"Tailored Experience\", style='Heading 2')\n",
        "    bullet_lines = bullet_points.split(\"\\n\")\n",
        "    for line in bullet_lines:\n",
        "        cleaned_line = line.strip()\n",
        "        if not is_job_title(cleaned_line):\n",
        "            cleaned_line = cleaned_line.lstrip('- ').strip()\n",
        "\n",
        "            if len(cleaned_line) > 3 and not re.match(r\"^\\s*[\\-\\•\\*\\d]+\\s*$\", cleaned_line):\n",
        "                doc.add_paragraph(cleaned_line, style='List Bullet')\n",
        "        else:\n",
        "            doc.add_paragraph(cleaned_line)\n",
        "\n",
        "    # Add Education section to the document\n",
        "    doc.add_paragraph(\"Education\", style='Heading 2')\n",
        "    education_lines = education.split(\"\\n\")\n",
        "    for line in education_lines:\n",
        "        if line.strip():\n",
        "            # Add bullet only if the line is not a project/continuation\n",
        "            if not (\"Project\" in line or \"Distinction\" in line or \"Honors\" in line):\n",
        "                doc.add_paragraph(line.strip(), style='List Bullet')\n",
        "            else:\n",
        "                doc.add_paragraph(line.strip())\n",
        "\n",
        "    # for line in education_lines:\n",
        "    #     if line.strip() and is_education_background:\n",
        "    #         doc.add_paragraph(line.strip(), style='List Bullet')\n",
        "\n",
        "    # Save the document\n",
        "    doc.save(file_name)\n",
        "    print(f\"Document saved as {file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note*: Function for **Education Section extraction** was adopted and modified from https://github.com/avr2002/CV-JD-Matching/blob/main/01_pdf-data-extraction.ipynb"
      ],
      "metadata": {
        "id": "KQl9Ae7hxYN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def extract_education_bert(pdf_path):\n",
        "    nlp = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "    lines = extract_text_from_pdf(pdf_path).split('\\n')\n",
        "    entities = nlp(lines)\n",
        "\n",
        "    extracted_education = \"\"\n",
        "\n",
        "    capturing = False\n",
        "\n",
        "    end_section_keywords = [\"Certifications\", \"Certificate\", \"Certified\", \"Work Experience\",\n",
        "                            \"Referees\", \"Skills\", \"Publications\", \"Courses\", \"Conferences\"]\n",
        "    end_section_keywords = [keyword.lower() for keyword in end_section_keywords]\n",
        "\n",
        "    degree_keywords = [\"University\", \"Bachelor\", \"Master\", \"Phd\", \"Doctorate\",\n",
        "                      \"M.Sc\", \"MSc\", \"B.Sc\", \"BSc\", \"MBA\", \"Diploma\", \"Associate\", \"College\",\n",
        "                      \"School\", \"Institute\", \"Academy\"]\n",
        "    degree_keywords = [keyword.lower() for keyword in degree_keywords]\n",
        "    referee_keywords = [\"Referees\", \"Reference\"]\n",
        "    referee_keywords = [keyword.lower() for keyword in referee_keywords]\n",
        "\n",
        "    current_entry = []\n",
        "    in_referees_section = False\n",
        "\n",
        "    for line, entity_list in zip(lines, entities):\n",
        "        line_clean = line.strip()\n",
        "        if not line_clean:\n",
        "            continue\n",
        "\n",
        "        if any(keyword in line.lower() for keyword in referee_keywords):\n",
        "            in_referees_section = True\n",
        "\n",
        "        if in_referees_section:\n",
        "            continue\n",
        "\n",
        "        if any(keyword in line.lower() for keyword in end_section_keywords):\n",
        "            capturing = False\n",
        "\n",
        "        if any(keyword in line.lower() for keyword in degree_keywords):\n",
        "            capturing = True\n",
        "\n",
        "        if capturing:\n",
        "\n",
        "            entity_text = \" \".join([entity['word'] for entity in entity_list if entity[\"entity\"] in [\"B-ORG\", \"I-ORG\"]])\n",
        "            if entity_text:\n",
        "                current_entry.append(line_clean)\n",
        "            elif current_entry:\n",
        "                extracted_education += \" \".join(current_entry) + \"\\n\"\n",
        "                current_entry = []\n",
        "\n",
        "    if current_entry:\n",
        "        extracted_education += \" \".join(current_entry) + \"\\n\"\n",
        "\n",
        "    return extracted_education if extracted_education else \"No education background found\""
      ],
      "metadata": {
        "id": "Im_vchbNGAiu"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUqexmyQKBVc",
        "outputId": "857c8311-75db-48e1-e18c-f354aedbe2bc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140096"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Lh0qv2VZmdIj"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    model_path = \"nakamoto-yama/t5-resume-generation\"\n",
        "    # model_path = \"google/gemma-2b-it\"\n",
        "    print(f\"Loading model and tokenizer from {model_path}\")\n",
        "\n",
        "    tokenizer, model = load_model_and_tokenizer(model_path)\n",
        "\n",
        "    # Process the experience in resume and summarize it\n",
        "    experience_summary = process_resume(pdf_path, \"openai\", api_key)\n",
        "\n",
        "    resume_summary = process_resume(pdf_path, \"huggingface\", api_key)\n",
        "\n",
        "\n",
        "    # Test the model with a prompt to generate job description\n",
        "    prompt = input(\"Enter a job description or title: \")\n",
        "    response = generate_text(f\"refine this resume {experience_summary} to match the following job description: {prompt}\", tokenizer, model)\n",
        "\n",
        "    response = response.replace(\"LB>\", \"{\").replace(\"RB>\", \"}\")\n",
        "    print(f\"Generated Response: {response}\")\n",
        "\n",
        "    # Extract key phrases from the job description\n",
        "    key_phrases = extract_key_phrases(prompt)\n",
        "    print(f\"Extracted Key Phrases: {key_phrases}\")\n",
        "\n",
        "    # Match experience in resume text based on key words\n",
        "    matched_experience = match_experience_to_job(key_phrases, experience_summary)\n",
        "\n",
        "    # Format the matched experience as bullet points\n",
        "    bullet_points = format_bullet_points(matched_experience)\n",
        "\n",
        "    education = extract_education_bert(pdf_path)\n",
        "\n",
        "    print(f\"\\nExtracted Education: {education}\")\n",
        "    # print(education)\n",
        "\n",
        "    print(\"\\nGenerated Experience Bullet Points:\\n\")\n",
        "    print(bullet_points)\n",
        "\n",
        "    # Generate a brief text based summary based on the bullet points\n",
        "    prompt2 = \"Generate a brief profile based on the following resume experience: \" + bullet_points\n",
        "    openai_generated_text = openai_generate_text(prompt2, api_key)\n",
        "    # generate_text_with_gemma = gemma_generate_text(prompt2)\n",
        "\n",
        "    if openai_generated_text:\n",
        "        # print(\"\\nSummary:\\n\")\n",
        "        print(openai_generated_text)\n",
        "\n",
        "\n",
        "    # Create a Word document with the bullet points and additional achievements\n",
        "    create_resume_doc(bullet_points, education, openai_generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}