{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Help was taken from ChatGPT to generate some of the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZA4p5VseC39"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNjRSK9IeOZF",
        "outputId": "a853345a-b83a-4a2d-c01d-000f4b23d415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_0vmugBmT11",
        "outputId": "6346c034-34c9-4039-acc0-1e5c912c2ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-3.1.1\n",
            "Collecting pipeline\n",
            "  Downloading pipeline-0.1.0-py3-none-any.whl.metadata (483 bytes)\n",
            "Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n",
            "Installing collected packages: pipeline\n",
            "Successfully installed pipeline-0.1.0\n",
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
            "Collecting configobj (from fitz)\n",
            "  Downloading configobj-5.0.9.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting configparser (from fitz)\n",
            "  Downloading configparser-7.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (5.2.1)\n",
            "Collecting nipype (from fitz)\n",
            "  Downloading nipype-1.8.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.1.4)\n",
            "Collecting pyxnat (from fitz)\n",
            "  Downloading pyxnat-1.6.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.13.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.1.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (24.1)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.7)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.3)\n",
            "Collecting prov>=1.5.2 (from nipype->fitz)\n",
            "  Downloading prov-2.0.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting simplejson>=3.8.0 (from nipype->fitz)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n",
            "  Downloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.16.1)\n",
            "Collecting etelemetry>=0.2.0 (from nipype->fitz)\n",
            "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting looseversion (from nipype->fitz)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.1)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (4.9.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (2.32.3)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.16.0)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2024.8.30)\n",
            "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading configparser-7.1.0-py3-none-any.whl (17 kB)\n",
            "Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
            "Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: configobj\n",
            "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.9-py2.py3-none-any.whl size=35614 sha256=79a774dba38a4100e23a31dc9c8f6d289f42b805ead052f8f207e3c65855044c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6c/03/6c5e3cf1a6e4b9e2fc5c4409be4abc5a8268bd9c878739cb32\n",
            "Successfully built configobj\n",
            "Installing collected packages: looseversion, traits, simplejson, isodate, configparser, configobj, ci-info, rdflib, pyxnat, etelemetry, prov, nipype, fitz\n",
            "Successfully installed ci-info-0.3.0 configobj-5.0.9 configparser-7.1.0 etelemetry-0.3.1 fitz-0.0.1.dev2 isodate-0.6.1 looseversion-1.3.0 nipype-1.8.6 prov-2.0.1 pyxnat-1.6.2 rdflib-6.3.2 simplejson-3.19.3 traits-6.3.2\n",
            "Collecting frontend\n",
            "  Downloading frontend-0.0.3-py3-none-any.whl.metadata (847 bytes)\n",
            "Collecting starlette>=0.12.0 (from frontend)\n",
            "  Downloading starlette-0.39.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting uvicorn>=0.7.1 (from frontend)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.2.0)\n",
            "Collecting aiofiles (from frontend)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.7.1->frontend)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.2.2)\n",
            "Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
            "Downloading starlette-0.39.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, aiofiles, uvicorn, starlette, frontend\n",
            "Successfully installed aiofiles-24.1.0 frontend-0.0.3 h11-0.14.0 starlette-0.39.1 uvicorn-0.31.0\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=d79d3591ec813aea7bdb561a99cdae00d91722c9246ce73f10e596a5a4b03124\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install pipeline\n",
        "!pip install fitz\n",
        "!pip install frontend\n",
        "!pip install docx2txt\n",
        "!pip install PyPDF2\n",
        "!pip install keybert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IAtkZVqjsVR"
      },
      "outputs": [],
      "source": [
        "import docx2txt\n",
        "import PyPDF2\n",
        "from transformers import pipeline\n",
        "import re\n",
        "from keybert import KeyBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXLC70g2rtOF"
      },
      "source": [
        "## Resume Text extraction from the sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POM2i8ZDrsyB"
      },
      "outputs": [],
      "source": [
        "def extract_text(file_path):\n",
        "    if file_path.endswith(\".docx\"):\n",
        "        # Extract text from DOCX file\n",
        "        return docx2txt.process(file_path)\n",
        "\n",
        "    elif file_path.endswith(\".pdf\"):\n",
        "        # Extract text from PDF file\n",
        "        text = \"\"\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "        return text\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7Sf4mufBEwQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_extracted_text(text):\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    # Fix misplaced hyphens and split words\n",
        "    text = re.sub(r'\\b-\\s+', '', text)\n",
        "    # Remove line breaks if they are not ending sections\n",
        "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
        "    # Fix instances of split words (e.g., \"D atabase\" -> \"Database\")\n",
        "    text = re.sub(r'\\b(\\w)\\s+(\\w)\\b', r'\\1\\2', text)\n",
        "    # Remove any email, phone, and URLs if needed\n",
        "    text = re.sub(r'http\\S+', '', text)  # URLs\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # Email addresses\n",
        "    text = re.sub(r'\\b\\d{3}[-.\\s]??\\d{3}[-.\\s]??\\d{4}\\b', '', text)  # Phone numbers\n",
        "    # Remove extra line breaks\n",
        "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbDUZ9omnv0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "Resume_text=extract_text(\"/content/drive/MyDrive/Resume/Anushka_Bhat_latest.docx\")\n",
        "job_des=extract_text(\"/content/drive/MyDrive/Resume/Job_description.docx\")\n",
        "\n",
        "cleaned_text = clean_extracted_text(Resume_text)\n",
        "cleaned_text_jd = clean_extracted_text(job_des)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKS4NAiGMng3"
      },
      "source": [
        "## Extract key words using keyBERT and check the similarity score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPQ7yM7-MplQ"
      },
      "outputs": [],
      "source": [
        "# Load KeyBERT model\n",
        "kw_model = KeyBERT('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract keywords from resume and job description\n",
        "resume_keywords = kw_model.extract_keywords(cleaned_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=15)\n",
        "job_description_keywords = kw_model.extract_keywords(cleaned_text_jd, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=15)\n",
        "\n",
        "# Convert keywords to a single string (focused text)\n",
        "focused_resume_text = ' '.join([kw[0] for kw in resume_keywords])\n",
        "focused_jd_text = ' '.join([kw[0] for kw in job_description_keywords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyPWuoz70mTF"
      },
      "outputs": [],
      "source": [
        "# Step 6: Example job description to compare with\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXj579jT8u-v"
      },
      "outputs": [],
      "source": [
        "job_embedding = sbert_model.encode(focused_jd_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAAdkGmp7_er"
      },
      "outputs": [],
      "source": [
        "resume_embedding = sbert_model.encode(focused_resume_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6EQ2hAe7HFz",
        "outputId": "18670ac7-0a44-4a2e-bbd7-4b1141ecdfd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score between job description and resume: 0.27\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Calculate similarity for each section\n",
        "similarity_scores = {}\n",
        "\n",
        "# Calculate cosine similarity between job description and resume embeddings\n",
        "similarity_score = util.pytorch_cos_sim(job_embedding, resume_embedding).item()\n",
        "\n",
        "# Print similarity score\n",
        "print(f\"Similarity score between job description and resume: {similarity_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "SyCIktbr_MYL",
        "outputId": "41d9f458-d1d5-4fcf-d73d-bd27684f5f89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'manufacturing engineering engineering manufacturing processes manufacturing manufacturing equipment industrial engineering engineering industrial new manufacturing manufacturing eng lean manufacturing op manufacturing mechanical engineering job requirements manufacturing fit engineering manufacturing techniques'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "focused_jd_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "8WIzB2xsEcPx",
        "outputId": "c4e839c6-fcea-4035-b8ac-55c79e16aa98"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'database skills technology intern gaining expertise expertise machine business intelligence data engineers utilize ai learning capabilities currently graduate bhat graduate business engine skills knowledge computer science software engineer expertise'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "focused_resume_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "H-lGnEOPHw9i",
        "outputId": "8b45a0ed-a229-4046-f588-b3491b9c781a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Anushka Bhat Graduate Student, Department of Computer Science Purdue University, Fort Wayne, IN Email:  LinkedIn:anushkabhat07 Phone: 1- SUMMARY Over 7.5 years of experience in developing Database and Business Intelligence applications. Proven track in delivering high quality technical deliverables in data intensive environments. Currently a graduate student in Department of Computer Science at Purdue University Fort Wayne, with the motivation of further expanding my skills set with different tools and technologies. My current focus is on gaining expertise in Machine Learning and Deep Learning through targeted coursework. ACADEMIC COURSEWORK Master of science in Computer Science – Machine learning, Natural Language Processing, Software Engineering, Web Development, Database Management Systems, Corporate Partners | GPA 4.0 2023-2025 Bachelor of Engineering in Computer Science | First class 2011-2015 LANGUAGES and TECHNOLOGIES Azure SQL, T-SQL, Python, HTML, CSS, JavaScript Azure Data Factory, MSBI (SSIS, SSRS, SSAS), Power BI, Alteryx WORK EXPERIENCE (7.5+ YEARS) Ernst & Young Texas, USA, Tax Technology Intern Jun 2024 – Aug 2024 Roles & Responsibilities: Leveraged database skills and knowledge in Artificial Intelligence to enhance internal custom tools designed for Tax technology, making them more efficient and client friendly. Assessed the overall quality of various Tax technology platforms and presented my own ideas for a better design leading to various tool fixes and enhancements. Worked on data visualization in Power BI by integrating and cleaning the data and then preparing it for visualization. Engaged in prompt engineering to utilize AI tools for automating various client processes, reducing the need for manual intervention. Ernst & Young GDS, India, Tax Senior Sept 2019 – Aug 2023 Roles & Responsibilities: Taking leadership on the assigned modules, proposing solutions, design and implementation while also translating the business requirements to the dev and QA team members. Analyzed business requirements and developed business engine for Electric vehicle incentive app, ensuring accurate tax credit calculations for electric vehicles and seamless user experience, directly coordinated with PO and stakeholders. Successfully oversaw the seamless migration of tax calculations from SQL server to Spark framework, resulting in significant time savings (by 40 minutes per execution) for processing large datasets. Key contributor in bridging the system gap and enhancing the global indirect tax platform by translating client requirements into actionable steps for multiple applications, resulting in improved efficiency and effectiveness. Bitwise Solutions, India, Programmer Analyst April 2018 – July 2019 Roles & Responsibilities: Leading a team of 4 data engineers, pairing with Product Owners to translate stakeholder requirements, collaborating with Architects to draft integration and system designs. Delivered fully automated data integrations for creation and generation of Payment Collaterals using SQL Server Integration services – emphasizing in speed resulting in overall reduction of execution time by 6 hours on daily basis. Tech Mahindra Ltd., India, Associate Software Engineer Aug 2015 – Mar 2018 Roles & Responsibilities: Design and Development, Source Data Analysis, Stakeholder Interactions, Production Deployments. Worked on data warehousing of Service Delivery data for an American Telecom using SSIS and Power BI to assist order tracking and dashboard creation, leading to transparent reporting of KPIs and enabling the management take better governing decisions. Drastically reduced the execution time (close to 3 hours daily) of Analysis services by eliminating redundant SQL executions and eliminating time intensive processes with more efficient database operations. REWARDS AND RECOGNITION SPOT Award - For Excellent performance, ownership, and quality throughout the year 2021. Extra Miler - For demonstrating quick learning capabilities and result-oriented performance in the quarter Jan-Mar 2020.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W3S-zxBMZKc"
      },
      "source": [
        "## Check with cosine similarity using TF-IDF vectoriser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF_wAndyJKBZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8cf7PDaLjVK",
        "outputId": "2b06dc6d-98e1-40f7-855d-ed6e08cce3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Cosine Similarity Score: 0.09\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Combine the cleaned texts into a corpus\n",
        "corpus = [cleaned_text, cleaned_text_jd]\n",
        "\n",
        "# Create the TF-IDF Vectorizer and fit-transform the corpus\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Calculate the cosine similarity between the resume and job description\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "tfidf_similarity_score = similarity_matrix[0][0]\n",
        "\n",
        "print(f\"TF-IDF Cosine Similarity Score: {tfidf_similarity_score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpBN2Kw4P26j"
      },
      "source": [
        "## check weighted cosine similairty with sbert and tf-idf vectoriser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpokcEGGPf66"
      },
      "outputs": [],
      "source": [
        "job_embedding = sbert_model.encode(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldIxmym8Pgua"
      },
      "outputs": [],
      "source": [
        "\n",
        "resume_embedding = sbert_model.encode(cleaned_text_jd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIvX_3C_Pidn"
      },
      "outputs": [],
      "source": [
        "Sbert_similarity_score = util.pytorch_cos_sim(job_embedding, resume_embedding).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHwMnO3mLmQW",
        "outputId": "e5dd1df7-7a80-4d39-deba-79c7503b878b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Similarity Score: 0.17\n"
          ]
        }
      ],
      "source": [
        "# Weighted combination of TF-IDF and SBERT scores\n",
        "final_similarity_score = 0.7 * tfidf_similarity_score + 0.3 * Sbert_similarity_score\n",
        "print(f\"Combined Similarity Score: {final_similarity_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xQP93s5QBD3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
