{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEIOMCUYk6nr",
        "outputId": "8acc3b85-f415-4261-f3c5-a8f1aa831e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4armtw1Sk_Jt"
      },
      "outputs": [],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voQ2eGw02atY",
        "outputId": "6f2ecf52-5794-4140-f6cf-86d3d2b01762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-yZUm5uvm9q",
        "outputId": "236e0605-40ab-4a56-ce36-ca5060a608ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq transformers datasets accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHM9TrB5D0dv",
        "outputId": "ed345a35-a78b-473a-a8b2-f8dca95efbaa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/471.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m419.8/471.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install quietly\n",
        "!pip install huggingface_hub -qq"
      ],
      "metadata": {
        "id": "qWukDMTwbzmn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_D5otMGFUiZ",
        "outputId": "6c3313af-855e-4dd1-881a-e21a92ef4fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "from typing import List\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that you export the environment variable before running the code\n",
        "# HF_KEY_FINE = userdata.get(\"HF_KEY_FINE\")  # Set this with your actual token before running\n",
        "# login(token=os.environ.get(\"HF_KEY_FINE\"))"
      ],
      "metadata": {
        "id": "KOmLBMcydIfT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_ACCESS_TOKEN')"
      ],
      "metadata": {
        "id": "_XSQ3SqXdqOB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZltpasEXmuXD"
      },
      "outputs": [],
      "source": [
        "# Function to generate text using OpenAI API\n",
        "def openai_generate_text(prompt, api_key, model=\"gpt-4\", max_tokens=100, temperature=0.7):\n",
        "    openai.api_key = api_key\n",
        "\n",
        "    try:\n",
        "        # Call OpenAI's GPT model to generate text using Chat API\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "\n",
        "        # Extract generated text from response\n",
        "        generated_text = response['choices'][0]['message']['content'].strip()\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "FkkazscbD8oN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemma_generate_text(prompt, model=\"google/gemma-2b-it\", max_tokens=100,):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Use pipeline API\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        model_kwargs={\"torch_dtype\": \"auto\"},\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # Delete large objects\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate the text with a maximum of 256 new tokens\n",
        "    response = pipe(prompt, max_new_tokens=max_tokens)\n",
        "\n",
        "    # Extract the generated text from the response\n",
        "    generated_response = response[0][\"generated_text\"].strip()\n",
        "    return generated_response"
      ],
      "metadata": {
        "id": "fv66tOD3BO5N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MWKdCHGbmrVq"
      },
      "outputs": [],
      "source": [
        "# Load the T5 model and tokenizer\n",
        "def load_model_and_tokenizer(model_path):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Gemma model and tokenizer\n",
        "def load_gemma_model_and_tokenizer(model_path):\n",
        "    login(token=HF_TOKEN)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    return tokenizer, model"
      ],
      "metadata": {
        "id": "s5aWVnE7BIbl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GADAEbHambTz"
      },
      "outputs": [],
      "source": [
        "# Generate text using the model based on the given prompt\n",
        "def generate_text(prompt, tokenizer, model):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids\n",
        "    outputs = model.generate(input_ids, max_length=512, num_return_sequences=1)\n",
        "    # outputs = model.generate(input_ids, max_length=512, num_return_sequences=1, encoder_outputs=None)\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "myWRXTqrmkUg"
      },
      "outputs": [],
      "source": [
        "# Function to extract relevant key words/phrases from response\n",
        "def extract_key_words(response: str) -> List[str]:\n",
        "    key_words = [word.strip() for word in response.split() if len(word) > 3]\n",
        "    return key_words\n",
        "\n",
        "# Function to extract key words/phrases from job description\n",
        "def extract_key_phrases(job_description: str) -> List[str]:\n",
        "    key_words = [word.strip() for word in job_description.split() if len(word) > 3]\n",
        "    return key_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NcT6orqVmh2p"
      },
      "outputs": [],
      "source": [
        "# Function to find matching experience in resume text\n",
        "# def match_experience_in_resume(key_words: List[str], resume_txt: str) -> List[str]:\n",
        "#     matched_experiences = set()\n",
        "#     for word in key_words:\n",
        "#         if word.lower() in resume_txt.lower():\n",
        "#             # Find sentence containing the word in resume\n",
        "#             sentences = resume_txt.split(\". \")\n",
        "#             for sentence in sentences:\n",
        "#                 if word.lower() in sentence.lower():\n",
        "#                     matched_experiences.add(sentence.strip())\n",
        "#     return list(matched_experiences)\n",
        "\n",
        "\n",
        "# Function to find matching experience in resume text based on job description key phrases\n",
        "def match_experience_to_job(key_phrases: List[str], resume_txt: str) -> List[str]:\n",
        "    matched_experiences = set()\n",
        "    sentences = resume_txt.split(\". \")\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for phrase in key_phrases:\n",
        "            if phrase.lower() in sentence.lower():\n",
        "                matched_experiences.add(sentence.strip())\n",
        "                break\n",
        "    return list(matched_experiences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GEifRo1UmgRy"
      },
      "outputs": [],
      "source": [
        "# Format the matched experiences into bullet points\n",
        "def format_bullet_points(experiences: List[str]) -> str:\n",
        "    if not experiences:\n",
        "        return \"No matching experience found.\"\n",
        "    bullet_points = \"\\n\".join([f\"- {exp}\" for exp in experiences])\n",
        "    return bullet_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bO6ZhZWp2TaH"
      },
      "outputs": [],
      "source": [
        "#Summarize Resume\n",
        "import pdfplumber\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    all_text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            all_text += page.extract_text()\n",
        "    return all_text.strip()\n",
        "\n",
        "# Function to summarize the extracted text using HuggingFace T5 model\n",
        "def summarize_text(text: str, max_length: int = 150) -> str:\n",
        "    summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
        "    summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Using OpenAI API for summarization\n",
        "def summarize_experience(text: str, api_key: str, model=\"gpt-4\"):\n",
        "    openai.api_key = api_key\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            # messages=[{\"role\": \"user\", \"content\": f\"Summarize the work experience in this resume:\\n{text}\"}],\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"Summarize the work experience for each job position held in this resume:\\n{text}\"}],\n",
        "            max_tokens=300\n",
        "        )\n",
        "        summary = response['choices'][0]['message']['content'].strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract and summarize resume from PDF\n",
        "def process_resume(pdf_path: str, selection: str, api_key: str):\n",
        "    summarized_text = None\n",
        "    resume_text = extract_text_from_pdf(pdf_path)\n",
        "    #print(\"Extracted Resume Text:\\n\", resume_text)\n",
        "\n",
        "    if selection == \"openai\":\n",
        "        summarized_text = summarize_experience(resume_text, api_key)\n",
        "    elif selection == \"huggingface\":\n",
        "        summarized_text = summarize_text(resume_text)\n",
        "    else:\n",
        "        print(\"Invalid selection. Please choose 'openai' or 'huggingface'.\")\n",
        "\n",
        "    print(\"\\nSummarized Resume Text:\\n\", summarized_text)\n",
        "    return summarized_text\n",
        "\n",
        "\n",
        "pdf_path = \"Pauline Korukundo CV.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt, Inches\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "from docx.shared import Cm\n",
        "import re\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Y5HSSpuX7MMz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SpaCy to Extract Job Titles\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to check if a line is a job title using NER\n",
        "def is_job_title(line: str) -> bool:\n",
        "    doc = nlp(line)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"ORG\" or ent.label_ == \"JOB_TITLE\":\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "zkbRzlDt33zc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl9Jr_tkv1Sa"
      },
      "source": [
        "Prepare output document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZiANtG7av0RW"
      },
      "outputs": [],
      "source": [
        "def create_resume_doc(bullet_points: str, education: str, profile: str, file_name: str = \"Tailored_Resume.docx\"):\n",
        "    # Create a new Document\n",
        "    doc = Document()\n",
        "\n",
        "    # Set the document to A4 size\n",
        "    section = doc.sections[0]\n",
        "    section.page_height = Cm(29.7)\n",
        "    section.page_width = Cm(21.0)\n",
        "\n",
        "    # Set margins for A4\n",
        "    section.left_margin = Cm(2)\n",
        "    section.right_margin = Cm(2)\n",
        "    section.top_margin = Cm(2)\n",
        "    section.bottom_margin = Cm(2)\n",
        "\n",
        "    # Title of the Resume\n",
        "    title = doc.add_heading(\"RESUME\", level=1)\n",
        "    title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
        "\n",
        "    # Profile Section\n",
        "\n",
        "    if profile:\n",
        "        doc.add_paragraph(\"Profile\", style='Heading 2')\n",
        "        achievement_lines = profile.split(\"\\n\")\n",
        "        for line in achievement_lines:\n",
        "            if line.strip():\n",
        "                doc.add_paragraph(line.strip(), style='List Bullet')\n",
        "\n",
        "    # Add Experience Section to document\n",
        "    doc.add_paragraph(\"Tailored Experience\", style='Heading 2')\n",
        "    bullet_lines = bullet_points.split(\"\\n\")\n",
        "    for line in bullet_lines:\n",
        "        cleaned_line = line.strip()\n",
        "        # Use NER to recognize job titles\n",
        "        if not is_job_title(cleaned_line):\n",
        "            # Remove leading dashes or extra spaces from experience lines\n",
        "            cleaned_line = cleaned_line.lstrip('- ').strip()\n",
        "\n",
        "            # Only add lines that are not empty and not standalone numbers or bullets\n",
        "            if len(cleaned_line) > 3 and not re.match(r\"^\\s*[\\-\\•\\*\\d]+\\s*$\", cleaned_line):\n",
        "                doc.add_paragraph(cleaned_line, style='List Bullet')\n",
        "        else:\n",
        "            doc.add_paragraph(cleaned_line)\n",
        "\n",
        "    # # Experience to the document\n",
        "    # doc.add_paragraph(\"Tailored Experience\", style='Heading 2')\n",
        "    # bullet_lines = bullet_points.split(\"\\n\")\n",
        "    # for line in bullet_lines:\n",
        "    #     cleaned_line = line.strip()\n",
        "    #     if len(cleaned_line) > 3 and not re.match(r\"^\\s*[\\-\\•\\*\\d]+\\s*$\", line.strip()):\n",
        "    #         doc.add_paragraph(line.strip(), style='List Bullet')\n",
        "\n",
        "    # Add Education section to the document\n",
        "    doc.add_paragraph(\"Education\", style='Heading 2')\n",
        "    education_lines = education.split(\"\\n\")\n",
        "    for line in education_lines:\n",
        "        if line.strip():\n",
        "            doc.add_paragraph(line.strip(), style='List Bullet')\n",
        "\n",
        "    # Save the document\n",
        "    doc.save(file_name)\n",
        "    print(f\"Document saved as {file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note*: Function for **Education Section extraction** was adopted and modified from https://github.com/avr2002/CV-JD-Matching/blob/main/01_pdf-data-extraction.ipynb"
      ],
      "metadata": {
        "id": "KQl9Ae7hxYN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Education from resume\n",
        "def extract_education(resume_txt: str) -> str:\n",
        "    education_entries = []\n",
        "\n",
        "    lines = resume_txt.split('\\n')\n",
        "    for line in lines:\n",
        "        if is_education_background(line):\n",
        "            education_entries.append(line)\n",
        "\n",
        "    if education_entries:\n",
        "        education = '\\n'.join(education_entries)\n",
        "    else:\n",
        "        education = \"No education background found\"\n",
        "\n",
        "    return education\n",
        "\n",
        "# Function to check if a line is education background using NER\n",
        "def is_education_background(line: str) -> bool:\n",
        "    doc = nlp(line)\n",
        "\n",
        "    # Defining keywords for education\n",
        "    degree_keywords = [\"Bachelor\", \"Master\", \"Phd\", \"Doctorate\", \"M.Sc\", \"B.Sc\", \"MBA\", \"Diploma\", \"Associate\"]\n",
        "    has_degree_keyword = any(keyword in line for keyword in degree_keywords)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"ORG\" and has_degree_keyword or has_degree_keyword:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "pEsMF-wjxHwF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUqexmyQKBVc",
        "outputId": "c57809f3-b465-4cd0-d43b-54a029fe00e0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2026"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Lh0qv2VZmdIj"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    model_path = \"nakamoto-yama/t5-resume-generation\"\n",
        "    # model_path = \"google/gemma-2b-it\"\n",
        "    print(f\"Loading model and tokenizer from {model_path}\")\n",
        "\n",
        "    tokenizer, model = load_model_and_tokenizer(model_path)\n",
        "    # tokenizer, model = load_gemma_model_and_tokenizer(model_path)\n",
        "\n",
        "    # Process the experience in resume and summarize it\n",
        "    experience_summary = process_resume(pdf_path, \"openai\", api_key)\n",
        "    # print(\"\\nResume Summary:\\n\", resume_summary)\n",
        "\n",
        "    resume_summary = process_resume(pdf_path, \"huggingface\", api_key)\n",
        "\n",
        "\n",
        "    # Test the model with a prompt to generate job description\n",
        "    prompt = input(\"Enter a job description or title: \")\n",
        "    response = generate_text(f\"extract key phrases for a candidate matching the following job: {prompt}\", tokenizer, model)\n",
        "\n",
        "    response = response.replace(\"LB>\", \"{\").replace(\"RB>\", \"}\")\n",
        "    print(f\"Generated Response: {response}\")\n",
        "\n",
        "    # Extract key words from the generated response\n",
        "    # key_words = extract_key_words(response)\n",
        "    # print(f\"Extracted Key Words: {key_words}\")\n",
        "\n",
        "    # Extract key phrases from the job description\n",
        "    key_phrases = extract_key_phrases(prompt)\n",
        "    print(f\"Extracted Key Phrases: {key_phrases}\")\n",
        "\n",
        "    # Match experience in resume text based on key words\n",
        "    # matched_experience = match_experience_in_resume(key_words, resume_summary)\n",
        "    matched_experience = match_experience_to_job(key_phrases, experience_summary)\n",
        "\n",
        "    # Format the matched experience as bullet points\n",
        "    bullet_points = format_bullet_points(matched_experience)\n",
        "\n",
        "    education = extract_education(resume_summary)\n",
        "    # print(education)\n",
        "\n",
        "    print(\"\\nGenerated Experience Bullet Points:\\n\")\n",
        "    print(bullet_points)\n",
        "\n",
        "    # Generate a brief text based summary based on the bullet points\n",
        "    prompt2 = \"Generate a brief profile based on the following resume experience: \" + bullet_points\n",
        "    openai_generated_text = openai_generate_text(prompt2, api_key)\n",
        "    # generate_text_with_gemma = gemma_generate_text(prompt2)\n",
        "\n",
        "    if openai_generated_text:\n",
        "        # print(\"\\nSummary:\\n\")\n",
        "        print(openai_generated_text)\n",
        "\n",
        "\n",
        "    # Create a Word document with the bullet points and additional achievements\n",
        "    create_resume_doc(bullet_points, education, openai_generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}